{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)\n",
    "---\n",
    "In this notebook, we will implement a DQN agent with OpenAI Gym's LunarLander-v3 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Configuration:\n",
      "  PyTorch Version: 2.10.0\n",
      "  MPS Available: True\n",
      "  CUDA Available: False\n",
      "  ✓ Using MPS backend (Metal Performance Shaders on M3)\n",
      "  Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch device and MPS availability for M3 MacBook acceleration\n",
    "print(\"PyTorch Configuration:\")\n",
    "print(f\"  PyTorch Version: {torch.__version__}\")\n",
    "print(f\"  MPS Available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"  ✓ Using MPS backend (Metal Performance Shaders on M3)\")\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"  Using CUDA GPU\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"  Using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install box2d-py --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v3')\n",
    "env.reset(seed=0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test Environment and Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing environment and rendering...\n",
      "Episode completed\n",
      "Total reward: -136.36\n",
      "Number of frames rendered: 117\n",
      "Frame shape: (400, 600, 3)\n",
      "Rendering is working!\n"
     ]
    }
   ],
   "source": [
    "# Close the old environment and create a new one with rendering enabled\n",
    "env.close()\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "\n",
    "# Test a single episode with rendering\n",
    "print(\"Testing environment and rendering...\")\n",
    "state, _ = env.reset(seed=42)\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "for step in range(200):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Get the frame before stepping\n",
    "    frame = env.render()\n",
    "    if frame is not None:\n",
    "        frames.append(frame)\n",
    "    \n",
    "    # Execute the action\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Episode completed\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "print(f\"Number of frames rendered: {len(frames)}\")\n",
    "print(f\"Frame shape: {frames[0].shape if frames else 'No frames'}\")\n",
    "print(f\"Rendering is working!\" if frames else \"✗ No frames rendered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DQN\n",
    "\n",
    "Run the code cell below to train the agent from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, terminated)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if terminated or truncated:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=260.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTER VARIANT: Aggressive learning rates and exploration decay\n",
    "def dqn_fast(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.985):\n",
    "    \"\"\"Faster DQN variant with aggressive hyperparameters\"\"\"\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state, _ = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, terminated)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 260.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_fast.pth')\n",
    "            break\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Uncomment to run faster variant:\n",
    "# scores_fast = dqn_fast()\n",
    "# plt.plot(np.arange(len(scores_fast)), scores_fast)\n",
    "# plt.ylabel('Score')\n",
    "# plt.xlabel('Episode #')\n",
    "# plt.title('Fast DQN Training')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT: Compare different hyperparameter configurations\n",
    "# Run each variant in sequence and compare training curves\n",
    "\n",
    "def compare_training_variants():\n",
    "    \"\"\"Train agents with different hyperparameter settings and compare results\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    variants = {\n",
    "        'baseline': {'eps_decay': 0.995, 'label': 'Baseline (eps_decay=0.995)'},\n",
    "        'faster_decay': {'eps_decay': 0.985, 'label': 'Faster Decay (eps_decay=0.985)'},\n",
    "        'fastest_decay': {'eps_decay': 0.975, 'label': 'Fastest Decay (eps_decay=0.975)'},\n",
    "    }\n",
    "    \n",
    "    for variant_name, config in variants.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training variant: {config['label']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Reinitialize agent for each variant\n",
    "        agent_variant = Agent(state_size=8, action_size=4, seed=0)\n",
    "        \n",
    "        scores = []\n",
    "        scores_window = deque(maxlen=100)\n",
    "        eps = 1.0\n",
    "        \n",
    "        for i_episode in range(1, 1001):  # 1000 episodes max for comparison\n",
    "            state, _ = env.reset()\n",
    "            score = 0\n",
    "            for t in range(1000):\n",
    "                action = agent_variant.act(state, eps)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                agent_variant.step(state, action, reward, next_state, terminated)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            scores_window.append(score)\n",
    "            scores.append(score)\n",
    "            eps = max(0.01, config['eps_decay'] * eps)\n",
    "            \n",
    "            if i_episode % 100 == 0:\n",
    "                print(f'Episode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}')\n",
    "            \n",
    "            if np.mean(scores_window) >= 260.0:\n",
    "                print(f\"\\nSolved in {i_episode-100} episodes!\")\n",
    "                break\n",
    "        \n",
    "        results[variant_name] = scores\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for variant_name, scores in results.items():\n",
    "        config = variants[variant_name]\n",
    "        plt.plot(np.arange(len(scores)), scores, label=config['label'], alpha=0.7)\n",
    "    \n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.xlabel('Episode #', fontsize=12)\n",
    "    plt.title('DQN Training: Hyperparameter Comparison', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run comparison:\n",
    "# variant_results = compare_training_variants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM HYPERPARAMETER CONFIGURATION\n",
    "# Modify these values to experiment with different agent settings\n",
    "\n",
    "BUFFER_SIZE_CUSTOM = int(1e5)   # Experience replay buffer size (100k is standard)\n",
    "BATCH_SIZE_CUSTOM = 128          # Increased from 64 for faster updates\n",
    "GAMMA_CUSTOM = 0.99              # Discount factor (keep at 0.99)\n",
    "TAU_CUSTOM = 1e-2                # Target network soft update rate (increased from 1e-3)\n",
    "LR_CUSTOM = 1e-3                 # Learning rate (increased from 5e-4 to 1e-3)\n",
    "UPDATE_EVERY_CUSTOM = 4          # How often to update the network\n",
    "\n",
    "print(f\"\"\"\n",
    "Custom Hyperparameter Configuration:\n",
    "=====================================\n",
    "BUFFER_SIZE: {BUFFER_SIZE_CUSTOM}\n",
    "BATCH_SIZE: {BATCH_SIZE_CUSTOM}\n",
    "GAMMA: {GAMMA_CUSTOM}\n",
    "TAU: {TAU_CUSTOM}\n",
    "LR: {LR_CUSTOM}\n",
    "UPDATE_EVERY: {UPDATE_EVERY_CUSTOM}\n",
    "\n",
    "Note: To use these hyperparameters, you need to modify dqn_agent.py\n",
    "and reinitialize the agent. See next cell for instructions.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions: How to Use Custom Hyperparameters\n",
    "\n",
    "To apply custom hyperparameters:\n",
    "\n",
    "1. **Edit dqn_agent.py** (lines 12-17):\n",
    "   ```python\n",
    "   BUFFER_SIZE = int(1e5)    # Change as needed\n",
    "   BATCH_SIZE = 128          # Increase for faster learning\n",
    "   GAMMA = 0.99\n",
    "   TAU = 1e-2                # Increase to update target network faster\n",
    "   LR = 1e-3                 # Increase learning rate\n",
    "   UPDATE_EVERY = 4\n",
    "   ```\n",
    "\n",
    "2. **Reinitialize the agent** by running this cell:\n",
    "   ```python\n",
    "   agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "   ```\n",
    "\n",
    "3. **Retrain** by running the training cell again.\n",
    "\n",
    "**Quick Tuning Guide:**\n",
    "- **Faster convergence**: Increase LR (1e-3), Increase TAU (1e-2), Increase BATCH_SIZE (128)\n",
    "- **More stable training**: Decrease LR, Decrease TAU, Keep BATCH_SIZE moderate\n",
    "- **Better exploration**: Decrease eps_decay (0.98 instead of 0.995)\n",
    "- **Wider network**: Edit model.py, change fc1_units and fc2_units to 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Agent Hyperparameters\n",
    "\n",
    "Edit the values below to experiment with different learning configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Experiment with Hyperparameters\n",
    "\n",
    "Try different hyperparameter combinations to speed up training. Below are some variants:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Image, display\n",
    "import imageio\n",
    "\n",
    "# Close the old environment and create a new one with rendering enabled\n",
    "env.close()\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "\n",
    "# load the weights from file if it exists\n",
    "if os.path.exists('checkpoint.pth'):\n",
    "    agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    print(\"Trained weights loaded successfully!\")\n",
    "else:\n",
    "    print(\"checkpoint.pth not found. Using current (untrained) agent weights.\")\n",
    "    print(\"Run cell 5 to train the agent first.\")\n",
    "\n",
    "# Watch the agent perform and save episodes as videos\n",
    "for episode in range(3):\n",
    "    state, _ = env.reset()\n",
    "    episode_frames = []\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(200):\n",
    "        action = agent.act(state)\n",
    "        frame = env.render()\n",
    "        if frame is not None:\n",
    "            episode_frames.append(frame)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Save episode as GIF\n",
    "    if episode_frames:\n",
    "        filename = f'/tmp/episode_{episode}.gif'\n",
    "        imageio.mimsave(filename, episode_frames, duration=0.05)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, Frames = {len(episode_frames)}\")\n",
    "        display(Image(filename=filename))\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore\n",
    "\n",
    "In this exercise, you have implemented a DQN agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task with discrete actions!\n",
    "- You may like to implement some improvements such as prioritized experience replay, Double DQN, or Dueling DQN! \n",
    "- Write a blog post explaining the intuition behind the DQN algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
